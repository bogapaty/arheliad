<!-- import aframe and then ar.js with image tracking / location based features -->
<script src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js"></script>
<script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

<!-- style for the loader -->
<style>
  .arjs-loader {
    height: 100%;
    width: 100%;
    position: absolute;
    top: 0;
    left: 0;
    background-color: rgba(0, 0, 0, 0.8);
    z-index: 9999;
    display: flex;
    justify-content: center;
    align-items: center;
  }

  .arjs-loader div {
    text-align: center;
    font-size: 1.25em;
    color: white;
  }
</style>

<meta name="apple-mobile-web-app-capable" content="yes">
<script>
        // We're going to register a custom event listener through a-frame that will fire
        // whenever a marker has entered the camera view and is found through ar.js
        AFRAME.registerComponent('registerevents', {
            init: function() {
                var marker = this.el;

                // Element emits events when found and lost
                marker.setAttribute('emitevents', 'true');

                marker.addEventListener('markerFound', function() {
                    // Alright, a marker has been found. Let's get the video element
                    var vid = document.getElementById('waterVideo');

                    // Make sure that the video a-frame object is visible
                    document.querySelector('#water').setAttribute('visible', true);

                    // Reset the video to the beginning and play it through
                    vid.currentTime = 0;
                    vid.play();

                    // Once the video has completed, we're going to hide the a-video element
                    // which will display the a-image element with the watch behind it
                    vid.addEventListener('ended', function(e) {
                        document.querySelector('#water').setAttribute('visible', false);
                    });
                });
            }
        });
    </script>
<body style="margin : 0px; overflow: hidden;">
<!-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -->
<div class="arjs-loader">
    <div>Loading, please wait...</div>
</div>

<!-- a-frame scene -->
<a-scene
        vr-mode-ui="enabled: false;"
        renderer="logarithmicDepthBuffer: true;"
        embedded
        arjs="trackingMethod: best; sourceType: webcam;debugUIEnabled: false;"
>
    <a-assets timeout="10000">
        <img id="watchImage" src="heliacell.jpg">
        <video id="waterVideo" loop="false" autoplay="false" src="helia.mp4" preload="auto"></video>
    </a-assets>
    <!-- a-nft is the anchor that defines an Image Tracking entity -->
    <!-- on 'url' use the path to the Image Descriptors created before. -->
    <!-- the path should end with the name without the extension e.g. if file is 'pinball.fset' the path should end with 'pinball' -->
    <a-nft
            registerevents
            type="nft"
            url="heliacellnft/heliacell2"
            smooth="true"
            smoothCount="10"
            smoothTolerance=".01"
            smoothThreshold="5"
    >
        <!-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -->
        <a-image id="watch" src="#watchImage" height="2" width="1" position="0 1 0"></a-image>
        <a-video id="water" src="#waterVideo" height="2" width="1" position="0 1 0" autoplay="false"></a-video>

    </a-nft>
    <!-- static camera that moves according to the device movemenents -->
    <a-entity camera></a-entity>
</a-scene>
</body>
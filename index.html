<!DOCTYPE html>
<html>
<head>
    <title>AR Demo</title>
    <script src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js"></script>
    <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <script>
        // We're going to register a custom event listener through a-frame that will fire
        // whenever a marker has entered the camera view and is found through ar.js
        AFRAME.registerComponent('registerevents', {
            init: function() {
                var marker = this.el;

                // Element emits events when found and lost
                marker.setAttribute('emitevents', 'true');

                marker.addEventListener('markerFound', function() {
                    // Alright, a marker has been found. Let's get the video element
                    var vid = document.getElementById('waterVideo');

                    // Make sure that the video a-frame object is visible
                    document.querySelector('#water').setAttribute('visible', true);

                    // Reset the video to the beginning and play it through
                    vid.currentTime = 0;
                    vid.play();

                    // Once the video has completed, we're going to hide the a-video element
                    // which will display the a-image element with the watch behind it
                    vid.addEventListener('ended', function(e) {
                        document.querySelector('#water').setAttribute('visible', false);
                    });
                });
            }
        });
    </script>
</head>


<body style="margin : 0px; overflow: hidden;">
<!-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -->
<div class="arjs-loader">
    <div>Loading, please wait...</div>
</div>

<!-- a-frame scene -->
<a-scene
        vr-mode-ui="enabled: false;"
        renderer="logarithmicDepthBuffer: true;"
        embedded
        arjs="trackingMethod: best; sourceType: webcam;debugUIEnabled: false;"
>
    <a-assets timeout="10000">
        <img id="watchImage" src="./heliacell.jpg">
        <video id="waterVideo" loop="false" autoplay="false" src="./helia.mp4" preload="auto"></video>
    </a-assets>
    <!-- a-nft is the anchor that defines an Image Tracking entity -->
    <!-- on 'url' use the path to the Image Descriptors created before. -->
    <!-- the path should end with the name without the extension e.g. if file is 'pinball.fset' the path should end with 'pinball' -->
    <a-nft
            registerevents
            type="nft"
            url="heliacellnft/heliacell2"
            smooth="true"
            smoothCount="10"
            smoothTolerance=".01"
            smoothThreshold="5"
    >
        <!-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -->
        <a-image id="watch" src="#watchImage" height="1" width="1" position="0 0 -0.2"></a-image>
        <a-video id="water" src="#waterVideo" height="1" width="1" position="0 0 -0.2" autoplay="false"></a-video>

    </a-nft>
    <!-- static camera that moves according to the device movemenents -->
    <a-entity camera></a-entity>
</a-scene>
</body>
</html>